[Unit]
Description=vLLM Dynamic API Server (Managed by Backend)
After=network.target
# Add nvidia-persistenced.service if needed. Ensure it's installed and enabled.
# Requires=nvidia-persistenced.service

[Service]
Type=simple
# User and Group should be the user running the backend/vLLM process
User=%USER%
# Group=%GROUP% # Optional: Specify group if needed

WorkingDirectory=%VLLM_HOME%

# Environment variables needed by the launcher and vLLM
Environment="VLLM_HOME=%VLLM_HOME%"
# Centralized cache location for HuggingFace models
Environment="HF_HOME=%VLLM_HOME%/.cache/huggingface"
# Set PYTHONPATH to the installation directory to help with relative imports in utils
Environment="PYTHONPATH=%VLLM_HOME%"
# Add other necessary environment variables like CUDA_VISIBLE_DEVICES if required
# Example: Environment="CUDA_VISIBLE_DEVICES=0,1"

# Execute the launcher script using the virtual environment's python
# The launcher reads active_model.txt and model_config.json to get dynamic args
ExecStart=%VLLM_HOME%/venv/bin/python %VLLM_HOME%/backend/utils/vllm_launcher.py

# Restart policy
Restart=on-failure
RestartSec=10 # Correct syntax: Just the number of seconds

# Resource limits (Optional but recommended)
# LimitNOFILE=65536 # Increase open file limit
# LimitNPROC=131072 # Increase process limit

# Timeout settings (Optional)
# TimeoutStartSec=300 # Allow more time for model loading on start
# TimeoutStopSec=60

[Install]
WantedBy=multi-user.target